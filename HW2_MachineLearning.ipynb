{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_MachineLearning",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WesleyAldridge/HW2_MachineLearning/blob/master/HW2_MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rt44vyY1xW9b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## HW2 - Convolutional Neural Network For CIFAR10 Data Set"
      ]
    },
    {
      "metadata": {
        "id": "Pfuun9431eEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Instructions from professor:\n",
        "\n",
        "\"The goal of this homework is to create a convolutional neural network for the CIFAR10 data set.\n",
        "\n",
        "Make sure that the data is divided into:\n",
        "\n",
        "- training set,\n",
        "- validation set\n",
        "- test set.\n",
        "\n",
        "Use the training set to train your neural networks. Evaluate their performance on the validation data set.\n",
        "\n",
        "After trying several different architectures, choose the one that performs best of the validation set.\n",
        "\n",
        "Train this final architecture on the data from the training set and validation set and evaluate its performance on the test set.\""
      ]
    },
    {
      "metadata": {
        "id": "l34GNy7tyNks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the CIFAR10 data set"
      ]
    },
    {
      "metadata": {
        "id": "qICX25UpB4W0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**train_images.shape = (50000, 32, 32, 3)**\n",
        "\n",
        "**test_images.shape  = (10000, 32, 32, 3) **\n",
        "\n",
        "50,000 train_images of shape 32x32 pixels, made up of 3 channels (R,G,B)\n",
        "\n",
        "10,000 test_images of shape 32x32 pixels, made up of 3 channels (R,G,B)"
      ]
    },
    {
      "metadata": {
        "id": "FSJyddqGexl5",
        "colab_type": "code",
        "outputId": "08e1ddd4-ce63-4fd8-c083-ce24311a9365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import models, layers\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "training_set = train_images[0:25000]\n",
        "validation_set = train_images[25000:50000]\n",
        "combined_set = train_images\n",
        "test_set = test_images\n",
        "\n",
        "\n",
        "train_labels = to_categorical(train_labels, 10)\n",
        "test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "training_labels = train_labels[0:25000]\n",
        "validation_labels = train_labels[25000:50000]\n",
        "combined_labels = train_labels\n",
        "\n",
        "training_set.shape, validation_set.shape, test_set.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 32, 32, 3), (25000, 32, 32, 3), (10000, 32, 32, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "NqoZq8yYBBvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Making CNN"
      ]
    },
    {
      "metadata": {
        "id": "wLs7MsrVKae1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example CNN from class notes:\n",
        "\n",
        "Input Images > Conv2d (ReLU) > MaxPool > Conv2d (ReLU) > MaxPool > Fully Connected > Fully Connected (output) \n",
        "\n",
        "This is two convolution modules (convolution(ReLU) + pooling) for feature extraction, and two fully connected layers for classification."
      ]
    },
    {
      "metadata": {
        "id": "8ES8_yvjAVfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CNN = models.Sequential()\n",
        "\n",
        "# \"To start, the CNN receives an input feature map: a 3-dimensional matrix, where\n",
        "# the size of the first two dimensions corresponds to the length and width of the\n",
        "# images in pixels, and the size of the third dimension is 3 (corresponding to the\n",
        "# 3 channels of a color image: red, green, and blue).\" - class notes\n",
        "\n",
        "# \"A convolution extracts tiles of the input feature map, and applies filters to them\n",
        "# to compute new features, producing an output feature map, or convolved feature\n",
        "# (which may have a different size and depth than the input feature map)\n",
        "# Convolutions are defined by two parameters:\n",
        "# Size of the tiles that are extracted (typically 3x3 or 5x5 pixels).\n",
        "# The depth of the output feature map, which corresponds to the number of filters\n",
        "# that are applied.\" - class notes\n",
        "\n",
        "# \"After each convolution operation, the CNN applies a Rectified Linear Unit (ReLU)\n",
        "# transformation to the convolved feature, in order to introduce nonlinearity into\n",
        "# the model\" - class notes\n",
        "CNN.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "\n",
        "# \"After ReLU comes a pooling step, in which the CNN downsamples the convolved feature\n",
        "# (to save on processing time), reducing the number of dimensions of the feature map,\n",
        "# while still preserving the most critical feature information.\" - class notes\n",
        "\n",
        "# \"size of the max-pooling filter is typically 2x2 pixels\" - from class notes\n",
        "# the class notes talk about using a stride of 2, so I will use that here as well:\n",
        "CNN.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2)) \n",
        "CNN.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "CNN.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2) )\n",
        "\n",
        "CNN.add(layers.Flatten())\n",
        "CNN.add(layers.Dense(512, activation='relu'))\n",
        "\n",
        "# \"Typically, the final fully-connected layer contains a softmax activation function,\n",
        "# which outputs a probability value from 0 to 1 for each of the classification labels\n",
        "# the model is trying to predict\" - class notes\n",
        "CNN.add(layers.Dense(10, activation='softmax')) \n",
        "#CNN.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2MGYWy7wXZZi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### \"Use the training set to train your neural networks. Evaluate their performance on the validation data set.\"\n",
        "\n",
        "###\"After trying several different architectures, choose the one that performs best on the validation set.\"\n",
        "\n",
        "-- instructions from professor"
      ]
    },
    {
      "metadata": {
        "id": "U3wmUq8ESbjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "c4787d01-a91c-4dec-96fb-526b46b35dcf"
      },
      "cell_type": "code",
      "source": [
        "CNN.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "CNN.fit(training_set, training_labels, epochs = 5, batch_size=32)\n",
        "score = CNN.evaluate(validation_set, validation_labels, batch_size=32)\n",
        "print()\n",
        "print(str(score[1] * 100) + \"% accuracy\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 9s 354us/step - loss: 0.5029 - acc: 0.8266\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 8s 329us/step - loss: 0.3684 - acc: 0.8745\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 8s 329us/step - loss: 0.2739 - acc: 0.9080\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 8s 326us/step - loss: 0.2033 - acc: 0.9304\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 8s 329us/step - loss: 0.1564 - acc: 0.9499\n",
            "25000/25000 [==============================] - 3s 138us/step\n",
            "\n",
            "70.34% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KVtwCiz4X_QE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### \"Train this final architecture on the data from the training set and validation set and evaluate its performance on the test set.\""
      ]
    },
    {
      "metadata": {
        "id": "Xe8mn1CiX-b1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "ec09e5f6-f27e-43d7-c203-9c2adf6c5c9e"
      },
      "cell_type": "code",
      "source": [
        "CNN.fit(combined_set, combined_labels, epochs = 5, batch_size=32)\n",
        "score = CNN.evaluate(test_set, test_labels, batch_size=32)\n",
        "print()\n",
        "print(str(score[1] * 100) + \"% accuracy\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 16s 322us/step - loss: 0.6735 - acc: 0.8018\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 16s 319us/step - loss: 0.5533 - acc: 0.8296\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 16s 313us/step - loss: 0.4917 - acc: 0.8458\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 16s 326us/step - loss: 0.4306 - acc: 0.8637\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 16s 320us/step - loss: 0.3878 - acc: 0.8771\n",
            "10000/10000 [==============================] - 1s 118us/step\n",
            "\n",
            "69.66% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}